{
    "docs": [
        {
            "location": "/", 
            "text": "The \nspeech1\n command-line interface \n(CLI)\n allows to use most features of the \nGoogle Speech\n service from the comfort of your terminal.\n\n\nBy default all output is printed to standard out, but flags can be set to direct it into a file independent of your shell's\ncapabilities. Errors will be printed to standard error, and cause the program's exit code to be non-zero.\n\n\nIf data-structures are requested, these will be returned as pretty-printed JSON, to be useful as input to other tools.\n\n\nEverything else about the \nSpeech\n API can be found at the\n\nofficial documentation site\n.\n\n\nInstallation and Source Code\n\n\nInstall the command-line interface with cargo using:\n\n\ncargo install google-speech1-cli\n\n\n\n\nFind the source code \non github\n.\n\n\nUsage\n\n\nThis documentation was generated from the \nSpeech\n API at revision \n20190627\n. The CLI is at version \n1.0.12\n.\n\n\nspeech1 [options]\n        operations\n                get \nname\n [-p \nv\n]... [-o \nout\n]\n                list [-p \nv\n]... [-o \nout\n]\n        projects\n                locations-operations-get \nname\n [-p \nv\n]... [-o \nout\n]\n                locations-operations-list \nname\n [-p \nv\n]... [-o \nout\n]\n        speech\n                longrunningrecognize (-r \nkv\n)... [-p \nv\n]... [-o \nout\n]\n                recognize (-r \nkv\n)... [-p \nv\n]... [-o \nout\n]\n  speech1 --help\n\nConfiguration:\n  [--scope \nurl\n]...\n            Specify the authentication a method should be executed in. Each scope\n            requires the user to grant this application permission to use it.\n            If unset, it defaults to the shortest scope url for a particular method.\n  --config-dir \nfolder\n\n            A directory into which we will store our persistent data. Defaults to\n            a user-writable directory that we will create during the first invocation.\n            [default: ~/.google-service-cli]\n  --debug\n            Output all server communication to standard error. `tx` and `rx` are placed\n            into the same stream.\n  --debug-auth\n            Output all communication related to authentication to standard error. `tx`\n            and `rx` are placed into the same stream.\n\n\n\n\n\nConfiguration\n\n\nThe program will store all persistent data in the \n~/.google-service-cli\n directory in \nJSON\n files prefixed with \nspeech1-\n.  You can change the directory used to store configuration with the \n--config-dir\n flag on a per-invocation basis.\n\n\nMore information about the various kinds of persistent data are given in the following paragraphs.\n\n\nAuthentication\n\n\nMost APIs require a user to authenticate any request. If this is the case, the \nscope\n determines the \nset of permissions granted. The granularity of these is usually no more than \nread-only\n or \nfull-access\n.\n\n\nIf not set, the system will automatically select the smallest feasible scope, e.g. when invoking a\nmethod that is read-only, it will ask only for a read-only scope. \nYou may use the \n--scope\n flag to specify a scope directly. \nAll applicable scopes are documented in the respective method's CLI documentation.\n\n\nThe first time a scope is used, the user is asked for permission. Follow the instructions given \nby the CLI to grant permissions, or to decline.\n\n\nIf a scope was authenticated by the user, the respective information will be stored as \nJSON\n in the configuration\ndirectory, e.g. \n~/.google-service-cli/speech1-token-\nscope-hash\n.json\n. No manual management of these tokens\nis necessary.\n\n\nTo revoke granted authentication, please refer to the \nofficial documentation\n.\n\n\nApplication Secrets\n\n\nIn order to allow any application to use Google services, it will need to be registered using the \n\nGoogle Developer Console\n. APIs the application may use are then enabled for it\none by one. Most APIs can be used for free and have a daily quota.\n\n\nTo allow more comfortable usage of the CLI without forcing anyone to register an own application, the CLI\ncomes with a default application secret that is configured accordingly. This also means that heavy usage\nall around the world may deplete the daily quota.\n\n\nYou can workaround this limitation by putting your own secrets file at this location: \n\n~/.google-service-cli/speech1-secret.json\n, assuming that the required \nspeech\n API \nwas enabled for it. Such a secret file can be downloaded in the \nGoogle Developer Console\n at \n\nAPIs \n auth -\n Credentials -\n Download JSON\n and used as is.\n\n\nLearn more about how to setup Google projects and enable APIs using the \nofficial documentation\n.\n\n\nDebugging\n\n\nEven though the CLI does its best to provide usable error messages, sometimes it might be desirable to know\nwhat exactly led to a particular issue. This is done by allowing all client-server communication to be \noutput to standard error \nas-is\n.\n\n\nThe \n--debug\n flag will print all client-server communication to standard error, whereas the \n--debug-auth\n flag\nwill cause all communication related to authentication to standard error.\nIf the \n--debug\n flag is set, error-results will be debug-printed, possibly yielding more information about the \nissue at hand.\n\n\nYou may consider redirecting standard error into a file for ease of use, e.g. \nspeech1 --debug \nresource\n \nmethod\n [options] 2\ndebug.txt\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#installation-and-source-code", 
            "text": "Install the command-line interface with cargo using:  cargo install google-speech1-cli  Find the source code  on github .", 
            "title": "Installation and Source Code"
        }, 
        {
            "location": "/#usage", 
            "text": "This documentation was generated from the  Speech  API at revision  20190627 . The CLI is at version  1.0.12 .  speech1 [options]\n        operations\n                get  name  [-p  v ]... [-o  out ]\n                list [-p  v ]... [-o  out ]\n        projects\n                locations-operations-get  name  [-p  v ]... [-o  out ]\n                locations-operations-list  name  [-p  v ]... [-o  out ]\n        speech\n                longrunningrecognize (-r  kv )... [-p  v ]... [-o  out ]\n                recognize (-r  kv )... [-p  v ]... [-o  out ]\n  speech1 --help\n\nConfiguration:\n  [--scope  url ]...\n            Specify the authentication a method should be executed in. Each scope\n            requires the user to grant this application permission to use it.\n            If unset, it defaults to the shortest scope url for a particular method.\n  --config-dir  folder \n            A directory into which we will store our persistent data. Defaults to\n            a user-writable directory that we will create during the first invocation.\n            [default: ~/.google-service-cli]\n  --debug\n            Output all server communication to standard error. `tx` and `rx` are placed\n            into the same stream.\n  --debug-auth\n            Output all communication related to authentication to standard error. `tx`\n            and `rx` are placed into the same stream.", 
            "title": "Usage"
        }, 
        {
            "location": "/#configuration", 
            "text": "The program will store all persistent data in the  ~/.google-service-cli  directory in  JSON  files prefixed with  speech1- .  You can change the directory used to store configuration with the  --config-dir  flag on a per-invocation basis.  More information about the various kinds of persistent data are given in the following paragraphs.", 
            "title": "Configuration"
        }, 
        {
            "location": "/#authentication", 
            "text": "Most APIs require a user to authenticate any request. If this is the case, the  scope  determines the \nset of permissions granted. The granularity of these is usually no more than  read-only  or  full-access .  If not set, the system will automatically select the smallest feasible scope, e.g. when invoking a\nmethod that is read-only, it will ask only for a read-only scope. \nYou may use the  --scope  flag to specify a scope directly. \nAll applicable scopes are documented in the respective method's CLI documentation.  The first time a scope is used, the user is asked for permission. Follow the instructions given \nby the CLI to grant permissions, or to decline.  If a scope was authenticated by the user, the respective information will be stored as  JSON  in the configuration\ndirectory, e.g.  ~/.google-service-cli/speech1-token- scope-hash .json . No manual management of these tokens\nis necessary.  To revoke granted authentication, please refer to the  official documentation .", 
            "title": "Authentication"
        }, 
        {
            "location": "/#application-secrets", 
            "text": "In order to allow any application to use Google services, it will need to be registered using the  Google Developer Console . APIs the application may use are then enabled for it\none by one. Most APIs can be used for free and have a daily quota.  To allow more comfortable usage of the CLI without forcing anyone to register an own application, the CLI\ncomes with a default application secret that is configured accordingly. This also means that heavy usage\nall around the world may deplete the daily quota.  You can workaround this limitation by putting your own secrets file at this location:  ~/.google-service-cli/speech1-secret.json , assuming that the required  speech  API \nwas enabled for it. Such a secret file can be downloaded in the  Google Developer Console  at  APIs   auth -  Credentials -  Download JSON  and used as is.  Learn more about how to setup Google projects and enable APIs using the  official documentation .", 
            "title": "Application Secrets"
        }, 
        {
            "location": "/#debugging", 
            "text": "Even though the CLI does its best to provide usable error messages, sometimes it might be desirable to know\nwhat exactly led to a particular issue. This is done by allowing all client-server communication to be \noutput to standard error  as-is .  The  --debug  flag will print all client-server communication to standard error, whereas the  --debug-auth  flag\nwill cause all communication related to authentication to standard error.\nIf the  --debug  flag is set, error-results will be debug-printed, possibly yielding more information about the \nissue at hand.  You may consider redirecting standard error into a file for ease of use, e.g.  speech1 --debug  resource   method  [options] 2 debug.txt .", 
            "title": "Debugging"
        }, 
        {
            "location": "/operations_get/", 
            "text": "Gets the latest state of a long-running operation.  Clients can use this\nmethod to poll the operation result at intervals as recommended by the API\nservice.\n\n\nScopes\n\n\nYou will need authorization for the \nhttps://www.googleapis.com/auth/cloud-platform\n scope to make a valid call.\n\n\nIf unset, the scope for this method defaults to \nhttps://www.googleapis.com/auth/cloud-platform\n.\nYou can set the scope for this method like this: \nspeech1 --scope \nscope\n operations get ...\n\n\nRequired Scalar Argument\n\n\n\n\nname\n \n(string)\n\n\nThe name of the operation resource.\n\n\n\n\n\n\n\n\nOptional Output Flags\n\n\nThe method's return value a JSON encoded structure, which will be written to standard output by default.\n\n\n\n\n-o out\n\n\nout\n specifies the \ndestination\n to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The \ndestination\n may be \n-\n to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.\n\n\n\n\n\n\n\n\nOptional General Properties\n\n\nThe following properties can configure any call, and are not specific to this method.\n\n\n\n\n\n\n-p $-xgafv=string\n\n\n\n\nV1 error format.\n\n\n\n\n\n\n\n\n-p access-token=string\n\n\n\n\nOAuth access token.\n\n\n\n\n\n\n\n\n-p alt=string\n\n\n\n\nData format for response.\n\n\n\n\n\n\n\n\n-p callback=string\n\n\n\n\nJSONP\n\n\n\n\n\n\n\n\n-p fields=string\n\n\n\n\nSelector specifying which fields to include in a partial response.\n\n\n\n\n\n\n\n\n-p key=string\n\n\n\n\nAPI key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.\n\n\n\n\n\n\n\n\n-p oauth-token=string\n\n\n\n\nOAuth 2.0 token for the current user.\n\n\n\n\n\n\n\n\n-p pretty-print=boolean\n\n\n\n\nReturns response with indentations and line breaks.\n\n\n\n\n\n\n\n\n-p quota-user=string\n\n\n\n\nAvailable to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.\n\n\n\n\n\n\n\n\n-p upload-type=string\n\n\n\n\nLegacy upload protocol for media (e.g. \nmedia\n, \nmultipart\n).\n\n\n\n\n\n\n\n\n-p upload-protocol=string\n\n\n\n\nUpload protocol for media (e.g. \nraw\n, \nmultipart\n).", 
            "title": "Get"
        }, 
        {
            "location": "/operations_get/#scopes", 
            "text": "You will need authorization for the  https://www.googleapis.com/auth/cloud-platform  scope to make a valid call.  If unset, the scope for this method defaults to  https://www.googleapis.com/auth/cloud-platform .\nYou can set the scope for this method like this:  speech1 --scope  scope  operations get ...", 
            "title": "Scopes"
        }, 
        {
            "location": "/operations_get/#required-scalar-argument", 
            "text": "name   (string)  The name of the operation resource.", 
            "title": "Required Scalar Argument"
        }, 
        {
            "location": "/operations_get/#optional-output-flags", 
            "text": "The method's return value a JSON encoded structure, which will be written to standard output by default.   -o out  out  specifies the  destination  to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The  destination  may be  -  to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.", 
            "title": "Optional Output Flags"
        }, 
        {
            "location": "/operations_get/#optional-general-properties", 
            "text": "The following properties can configure any call, and are not specific to this method.    -p $-xgafv=string   V1 error format.     -p access-token=string   OAuth access token.     -p alt=string   Data format for response.     -p callback=string   JSONP     -p fields=string   Selector specifying which fields to include in a partial response.     -p key=string   API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.     -p oauth-token=string   OAuth 2.0 token for the current user.     -p pretty-print=boolean   Returns response with indentations and line breaks.     -p quota-user=string   Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.     -p upload-type=string   Legacy upload protocol for media (e.g.  media ,  multipart ).     -p upload-protocol=string   Upload protocol for media (e.g.  raw ,  multipart ).", 
            "title": "Optional General Properties"
        }, 
        {
            "location": "/operations_list/", 
            "text": "Lists operations that match the specified filter in the request. If the\nserver doesn\nt support this method, it returns \nUNIMPLEMENTED\n.\n\n\nNOTE: the \nname\n binding allows API services to override the binding\nto use different resource name schemes, such as \nusers/*/operations\n. To\noverride the binding, API services can add a binding such as\n\n#34;/v1/{name=users/*}/operations\n#34;\n to their service configuration.\nFor backwards compatibility, the default name includes the operations\ncollection id, however overriding users must ensure the name binding\nis the parent resource, without the operations collection id.\n\n\nScopes\n\n\nYou will need authorization for the \nhttps://www.googleapis.com/auth/cloud-platform\n scope to make a valid call.\n\n\nIf unset, the scope for this method defaults to \nhttps://www.googleapis.com/auth/cloud-platform\n.\nYou can set the scope for this method like this: \nspeech1 --scope \nscope\n operations list ...\n\n\nOptional Output Flags\n\n\nThe method's return value a JSON encoded structure, which will be written to standard output by default.\n\n\n\n\n-o out\n\n\nout\n specifies the \ndestination\n to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The \ndestination\n may be \n-\n to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.\n\n\n\n\n\n\n\n\nOptional Method Properties\n\n\nYou may set the following properties to further configure the call. Please note that \n-p\n is followed by one \nor more key-value-pairs, and is called like this \n-p k1=v1 k2=v2\n even though the listing below repeats the\n\n-p\n for completeness.\n\n\n\n\n\n\n-p page-token=string\n\n\n\n\nThe standard list page token.\n\n\n\n\n\n\n\n\n-p page-size=integer\n\n\n\n\nThe standard list page size.\n\n\n\n\n\n\n\n\n-p filter=string\n\n\n\n\nThe standard list filter.\n\n\n\n\n\n\n\n\n-p name=string\n\n\n\n\nThe name of the operation\ns parent resource.\n\n\n\n\n\n\n\n\nOptional General Properties\n\n\nThe following properties can configure any call, and are not specific to this method.\n\n\n\n\n\n\n-p $-xgafv=string\n\n\n\n\nV1 error format.\n\n\n\n\n\n\n\n\n-p access-token=string\n\n\n\n\nOAuth access token.\n\n\n\n\n\n\n\n\n-p alt=string\n\n\n\n\nData format for response.\n\n\n\n\n\n\n\n\n-p callback=string\n\n\n\n\nJSONP\n\n\n\n\n\n\n\n\n-p fields=string\n\n\n\n\nSelector specifying which fields to include in a partial response.\n\n\n\n\n\n\n\n\n-p key=string\n\n\n\n\nAPI key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.\n\n\n\n\n\n\n\n\n-p oauth-token=string\n\n\n\n\nOAuth 2.0 token for the current user.\n\n\n\n\n\n\n\n\n-p pretty-print=boolean\n\n\n\n\nReturns response with indentations and line breaks.\n\n\n\n\n\n\n\n\n-p quota-user=string\n\n\n\n\nAvailable to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.\n\n\n\n\n\n\n\n\n-p upload-type=string\n\n\n\n\nLegacy upload protocol for media (e.g. \nmedia\n, \nmultipart\n).\n\n\n\n\n\n\n\n\n-p upload-protocol=string\n\n\n\n\nUpload protocol for media (e.g. \nraw\n, \nmultipart\n).", 
            "title": "List"
        }, 
        {
            "location": "/operations_list/#scopes", 
            "text": "You will need authorization for the  https://www.googleapis.com/auth/cloud-platform  scope to make a valid call.  If unset, the scope for this method defaults to  https://www.googleapis.com/auth/cloud-platform .\nYou can set the scope for this method like this:  speech1 --scope  scope  operations list ...", 
            "title": "Scopes"
        }, 
        {
            "location": "/operations_list/#optional-output-flags", 
            "text": "The method's return value a JSON encoded structure, which will be written to standard output by default.   -o out  out  specifies the  destination  to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The  destination  may be  -  to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.", 
            "title": "Optional Output Flags"
        }, 
        {
            "location": "/operations_list/#optional-method-properties", 
            "text": "You may set the following properties to further configure the call. Please note that  -p  is followed by one \nor more key-value-pairs, and is called like this  -p k1=v1 k2=v2  even though the listing below repeats the -p  for completeness.    -p page-token=string   The standard list page token.     -p page-size=integer   The standard list page size.     -p filter=string   The standard list filter.     -p name=string   The name of the operation s parent resource.", 
            "title": "Optional Method Properties"
        }, 
        {
            "location": "/operations_list/#optional-general-properties", 
            "text": "The following properties can configure any call, and are not specific to this method.    -p $-xgafv=string   V1 error format.     -p access-token=string   OAuth access token.     -p alt=string   Data format for response.     -p callback=string   JSONP     -p fields=string   Selector specifying which fields to include in a partial response.     -p key=string   API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.     -p oauth-token=string   OAuth 2.0 token for the current user.     -p pretty-print=boolean   Returns response with indentations and line breaks.     -p quota-user=string   Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.     -p upload-type=string   Legacy upload protocol for media (e.g.  media ,  multipart ).     -p upload-protocol=string   Upload protocol for media (e.g.  raw ,  multipart ).", 
            "title": "Optional General Properties"
        }, 
        {
            "location": "/projects_locations-operations-get/", 
            "text": "Gets the latest state of a long-running operation.  Clients can use this\nmethod to poll the operation result at intervals as recommended by the API\nservice.\n\n\nScopes\n\n\nYou will need authorization for the \nhttps://www.googleapis.com/auth/cloud-platform\n scope to make a valid call.\n\n\nIf unset, the scope for this method defaults to \nhttps://www.googleapis.com/auth/cloud-platform\n.\nYou can set the scope for this method like this: \nspeech1 --scope \nscope\n projects locations-operations-get ...\n\n\nRequired Scalar Argument\n\n\n\n\nname\n \n(string)\n\n\nThe name of the operation resource.\n\n\n\n\n\n\n\n\nOptional Output Flags\n\n\nThe method's return value a JSON encoded structure, which will be written to standard output by default.\n\n\n\n\n-o out\n\n\nout\n specifies the \ndestination\n to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The \ndestination\n may be \n-\n to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.\n\n\n\n\n\n\n\n\nOptional General Properties\n\n\nThe following properties can configure any call, and are not specific to this method.\n\n\n\n\n\n\n-p $-xgafv=string\n\n\n\n\nV1 error format.\n\n\n\n\n\n\n\n\n-p access-token=string\n\n\n\n\nOAuth access token.\n\n\n\n\n\n\n\n\n-p alt=string\n\n\n\n\nData format for response.\n\n\n\n\n\n\n\n\n-p callback=string\n\n\n\n\nJSONP\n\n\n\n\n\n\n\n\n-p fields=string\n\n\n\n\nSelector specifying which fields to include in a partial response.\n\n\n\n\n\n\n\n\n-p key=string\n\n\n\n\nAPI key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.\n\n\n\n\n\n\n\n\n-p oauth-token=string\n\n\n\n\nOAuth 2.0 token for the current user.\n\n\n\n\n\n\n\n\n-p pretty-print=boolean\n\n\n\n\nReturns response with indentations and line breaks.\n\n\n\n\n\n\n\n\n-p quota-user=string\n\n\n\n\nAvailable to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.\n\n\n\n\n\n\n\n\n-p upload-type=string\n\n\n\n\nLegacy upload protocol for media (e.g. \nmedia\n, \nmultipart\n).\n\n\n\n\n\n\n\n\n-p upload-protocol=string\n\n\n\n\nUpload protocol for media (e.g. \nraw\n, \nmultipart\n).", 
            "title": "Locations Operations Get"
        }, 
        {
            "location": "/projects_locations-operations-get/#scopes", 
            "text": "You will need authorization for the  https://www.googleapis.com/auth/cloud-platform  scope to make a valid call.  If unset, the scope for this method defaults to  https://www.googleapis.com/auth/cloud-platform .\nYou can set the scope for this method like this:  speech1 --scope  scope  projects locations-operations-get ...", 
            "title": "Scopes"
        }, 
        {
            "location": "/projects_locations-operations-get/#required-scalar-argument", 
            "text": "name   (string)  The name of the operation resource.", 
            "title": "Required Scalar Argument"
        }, 
        {
            "location": "/projects_locations-operations-get/#optional-output-flags", 
            "text": "The method's return value a JSON encoded structure, which will be written to standard output by default.   -o out  out  specifies the  destination  to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The  destination  may be  -  to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.", 
            "title": "Optional Output Flags"
        }, 
        {
            "location": "/projects_locations-operations-get/#optional-general-properties", 
            "text": "The following properties can configure any call, and are not specific to this method.    -p $-xgafv=string   V1 error format.     -p access-token=string   OAuth access token.     -p alt=string   Data format for response.     -p callback=string   JSONP     -p fields=string   Selector specifying which fields to include in a partial response.     -p key=string   API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.     -p oauth-token=string   OAuth 2.0 token for the current user.     -p pretty-print=boolean   Returns response with indentations and line breaks.     -p quota-user=string   Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.     -p upload-type=string   Legacy upload protocol for media (e.g.  media ,  multipart ).     -p upload-protocol=string   Upload protocol for media (e.g.  raw ,  multipart ).", 
            "title": "Optional General Properties"
        }, 
        {
            "location": "/projects_locations-operations-list/", 
            "text": "Lists operations that match the specified filter in the request. If the\nserver doesn\nt support this method, it returns \nUNIMPLEMENTED\n.\n\n\nNOTE: the \nname\n binding allows API services to override the binding\nto use different resource name schemes, such as \nusers/*/operations\n. To\noverride the binding, API services can add a binding such as\n\n#34;/v1/{name=users/*}/operations\n#34;\n to their service configuration.\nFor backwards compatibility, the default name includes the operations\ncollection id, however overriding users must ensure the name binding\nis the parent resource, without the operations collection id.\n\n\nScopes\n\n\nYou will need authorization for the \nhttps://www.googleapis.com/auth/cloud-platform\n scope to make a valid call.\n\n\nIf unset, the scope for this method defaults to \nhttps://www.googleapis.com/auth/cloud-platform\n.\nYou can set the scope for this method like this: \nspeech1 --scope \nscope\n projects locations-operations-list ...\n\n\nRequired Scalar Argument\n\n\n\n\nname\n \n(string)\n\n\nThe name of the operation\ns parent resource.\n\n\n\n\n\n\n\n\nOptional Output Flags\n\n\nThe method's return value a JSON encoded structure, which will be written to standard output by default.\n\n\n\n\n-o out\n\n\nout\n specifies the \ndestination\n to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The \ndestination\n may be \n-\n to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.\n\n\n\n\n\n\n\n\nOptional Method Properties\n\n\nYou may set the following properties to further configure the call. Please note that \n-p\n is followed by one \nor more key-value-pairs, and is called like this \n-p k1=v1 k2=v2\n even though the listing below repeats the\n\n-p\n for completeness.\n\n\n\n\n\n\n-p filter=string\n\n\n\n\nThe standard list filter.\n\n\n\n\n\n\n\n\n-p page-token=string\n\n\n\n\nThe standard list page token.\n\n\n\n\n\n\n\n\n-p page-size=integer\n\n\n\n\nThe standard list page size.\n\n\n\n\n\n\n\n\nOptional General Properties\n\n\nThe following properties can configure any call, and are not specific to this method.\n\n\n\n\n\n\n-p $-xgafv=string\n\n\n\n\nV1 error format.\n\n\n\n\n\n\n\n\n-p access-token=string\n\n\n\n\nOAuth access token.\n\n\n\n\n\n\n\n\n-p alt=string\n\n\n\n\nData format for response.\n\n\n\n\n\n\n\n\n-p callback=string\n\n\n\n\nJSONP\n\n\n\n\n\n\n\n\n-p fields=string\n\n\n\n\nSelector specifying which fields to include in a partial response.\n\n\n\n\n\n\n\n\n-p key=string\n\n\n\n\nAPI key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.\n\n\n\n\n\n\n\n\n-p oauth-token=string\n\n\n\n\nOAuth 2.0 token for the current user.\n\n\n\n\n\n\n\n\n-p pretty-print=boolean\n\n\n\n\nReturns response with indentations and line breaks.\n\n\n\n\n\n\n\n\n-p quota-user=string\n\n\n\n\nAvailable to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.\n\n\n\n\n\n\n\n\n-p upload-type=string\n\n\n\n\nLegacy upload protocol for media (e.g. \nmedia\n, \nmultipart\n).\n\n\n\n\n\n\n\n\n-p upload-protocol=string\n\n\n\n\nUpload protocol for media (e.g. \nraw\n, \nmultipart\n).", 
            "title": "Locations Operations List"
        }, 
        {
            "location": "/projects_locations-operations-list/#scopes", 
            "text": "You will need authorization for the  https://www.googleapis.com/auth/cloud-platform  scope to make a valid call.  If unset, the scope for this method defaults to  https://www.googleapis.com/auth/cloud-platform .\nYou can set the scope for this method like this:  speech1 --scope  scope  projects locations-operations-list ...", 
            "title": "Scopes"
        }, 
        {
            "location": "/projects_locations-operations-list/#required-scalar-argument", 
            "text": "name   (string)  The name of the operation s parent resource.", 
            "title": "Required Scalar Argument"
        }, 
        {
            "location": "/projects_locations-operations-list/#optional-output-flags", 
            "text": "The method's return value a JSON encoded structure, which will be written to standard output by default.   -o out  out  specifies the  destination  to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The  destination  may be  -  to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.", 
            "title": "Optional Output Flags"
        }, 
        {
            "location": "/projects_locations-operations-list/#optional-method-properties", 
            "text": "You may set the following properties to further configure the call. Please note that  -p  is followed by one \nor more key-value-pairs, and is called like this  -p k1=v1 k2=v2  even though the listing below repeats the -p  for completeness.    -p filter=string   The standard list filter.     -p page-token=string   The standard list page token.     -p page-size=integer   The standard list page size.", 
            "title": "Optional Method Properties"
        }, 
        {
            "location": "/projects_locations-operations-list/#optional-general-properties", 
            "text": "The following properties can configure any call, and are not specific to this method.    -p $-xgafv=string   V1 error format.     -p access-token=string   OAuth access token.     -p alt=string   Data format for response.     -p callback=string   JSONP     -p fields=string   Selector specifying which fields to include in a partial response.     -p key=string   API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.     -p oauth-token=string   OAuth 2.0 token for the current user.     -p pretty-print=boolean   Returns response with indentations and line breaks.     -p quota-user=string   Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.     -p upload-type=string   Legacy upload protocol for media (e.g.  media ,  multipart ).     -p upload-protocol=string   Upload protocol for media (e.g.  raw ,  multipart ).", 
            "title": "Optional General Properties"
        }, 
        {
            "location": "/speech_longrunningrecognize/", 
            "text": "Performs asynchronous speech recognition: receive results via the\ngoogle.longrunning.Operations interface. Returns either an\n\nOperation.error\n or an \nOperation.response\n which contains\na \nLongRunningRecognizeResponse\n message.\nFor more information on asynchronous speech recognition, see the\n\nhow-to\n.\n\n\nScopes\n\n\nYou will need authorization for the \nhttps://www.googleapis.com/auth/cloud-platform\n scope to make a valid call.\n\n\nIf unset, the scope for this method defaults to \nhttps://www.googleapis.com/auth/cloud-platform\n.\nYou can set the scope for this method like this: \nspeech1 --scope \nscope\n speech longrunningrecognize ...\n\n\nRequired Request Value\n\n\nThe request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure.\nIn the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely.\n\n\nFor example, a structure like this:\n\n\nLongRunningRecognizeRequest:\n  audio:\n    content: string\n    uri: string\n  config:\n    audio-channel-count: integer\n    enable-automatic-punctuation: boolean\n    enable-separate-recognition-per-channel: boolean\n    enable-word-time-offsets: boolean\n    encoding: string\n    language-code: string\n    max-alternatives: integer\n    metadata:\n      audio-topic: string\n      industry-naics-code-of-audio: integer\n      interaction-type: string\n      microphone-distance: string\n      obfuscated-id: string\n      original-media-type: string\n      original-mime-type: string\n      recording-device-name: string\n      recording-device-type: string\n    model: string\n    profanity-filter: boolean\n    sample-rate-hertz: integer\n    use-enhanced: boolean\n\n\n\n\n\ncan be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time.\n\n\n\n\n-r .audio    content=eirmod\n\n\nThe audio data bytes encoded as specified in\n    \nRecognitionConfig\n. Note: as with all bytes fields, proto buffers use a\n    pure binary representation, whereas JSON representations use base64.\n\n\n\n\n\n\n\n\nuri=sit\n\n\n\n\nURI that points to a file that contains audio data bytes as specified in\n    \nRecognitionConfig\n. The file must not be compressed (for example, gzip).\n    Currently, only Google Cloud Storage URIs are\n    supported, which must be specified in the following format:\n    \ngs://bucket_name/object_name\n (other URI formats return\n    google.rpc.Code.INVALID_ARGUMENT). For more information, see\n    \nRequest URIs\n.\n\n\n\n\n\n\n\n\n..config    audio-channel-count=36\n\n\n\n\nOptional\n The number of channels in the input audio data.\n    ONLY set this for MULTI-CHANNEL recognition.\n    Valid values for LINEAR16 and FLAC are \n1\n-\n8\n.\n    Valid values for OGG_OPUS are \n1\n-\n254\n.\n    Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only \n1\n.\n    If \n0\n or omitted, defaults to one channel (mono).\n    Note: We only recognize the first channel by default.\n    To perform independent recognition on each channel set\n    \nenable_separate_recognition_per_channel\n to \ntrue\n.\n\n\n\n\n\n\nenable-automatic-punctuation=true\n\n\nOptional\n If \ntrue\n, adds punctuation to recognition result hypotheses.\n    This feature is only available in select languages. Setting this for\n    requests in other languages has no effect at all.\n    The default \nfalse\n value does not add punctuation to result hypotheses.\n    Note: This is currently offered as an experimental service, complimentary\n    to all users. In the future this may be exclusively available as a\n    premium feature.\n\n\n\n\n\n\nenable-separate-recognition-per-channel=false\n\n\nThis needs to be set to \ntrue\n explicitly and \naudio_channel_count\n \n 1\n    to get each channel recognized separately. The recognition result will\n    contain a \nchannel_tag\n field to state which channel that result belongs\n    to. If this is not true, we will only recognize the first channel. The\n    request is billed cumulatively for all channels recognized:\n    \naudio_channel_count\n multiplied by the length of the audio.\n\n\n\n\n\n\nenable-word-time-offsets=true\n\n\nOptional\n If \ntrue\n, the top result includes a list of words and\n    the start and end time offsets (timestamps) for those words. If\n    \nfalse\n, no word-level time offset information is returned. The default is\n    \nfalse\n.\n\n\n\n\n\n\nencoding=kasd\n\n\nEncoding of audio data sent in all \nRecognitionAudio\n messages.\n    This field is optional for \nFLAC\n and \nWAV\n audio files and required\n    for all other audio formats. For details, see AudioEncoding.\n\n\n\n\n\n\nlanguage-code=accusam\n\n\nRequired\n The language of the supplied audio as a\n    \nBCP-47\n language tag.\n    Example: \nen-US\n.\n    See \nLanguage Support\n\n    for a list of the currently supported language codes.\n\n\n\n\n\n\nmax-alternatives=93\n\n\nOptional\n Maximum number of recognition hypotheses to be returned.\n    Specifically, the maximum number of \nSpeechRecognitionAlternative\n messages\n    within each \nSpeechRecognitionResult\n.\n    The server may return fewer than \nmax_alternatives\n.\n    Valid values are \n0\n-\n30\n. A value of \n0\n or \n1\n will return a maximum of\n    one. If omitted, will return a maximum of one.\n\n\n\n\n\n\nmetadata    audio-topic=justo\n\n\nDescription of the content. Eg. \nRecordings of federal supreme court\n    hearings from 2012\n.\n\n\n\n\n\n\nindustry-naics-code-of-audio=100\n\n\nThe industry vertical to which this speech recognition request most\n    closely applies. This is most indicative of the topics contained\n    in the audio.  Use the 6-digit NAICS code to identify the industry\n    vertical - see https://www.naics.com/search/.\n\n\n\n\n\n\ninteraction-type=erat\n\n\nThe use case most closely describing the audio content to be recognized.\n\n\n\n\n\n\nmicrophone-distance=labore\n\n\nThe audio type that most closely describes the audio being recognized.\n\n\n\n\n\n\nobfuscated-id=sea\n\n\nObfuscated (privacy-protected) ID of the user, to identify number of\n    unique users using the service.\n\n\n\n\n\n\noriginal-media-type=nonumy\n\n\nThe original media the speech was recorded on.\n\n\n\n\n\n\noriginal-mime-type=dolores\n\n\nMime type of the original audio file.  For example \naudio/m4a\n,\n    \naudio/x-alaw-basic\n, \naudio/mp3\n, \naudio/3gpp\n.\n    A list of possible audio mime types is maintained at\n    http://www.iana.org/assignments/media-types/media-types.xhtml#audio\n\n\n\n\n\n\nrecording-device-name=gubergren\n\n\nThe device used to make the recording.  Examples \nNexus 5X\n or\n    \nPolycom SoundStation IP 6000\n or \nPOTS\n or \nVoIP\n or\n    \nCardioid Microphone\n.\n\n\n\n\n\n\n\n\nrecording-device-type=sadipscing\n\n\n\n\nThe type of device the speech was recorded with.\n\n\n\n\n\n\n\n\n..    model=aliquyam\n\n\n\n\nOptional\n Which model to select for the given request. Select the model\n    best suited to your domain to get best results. If a model is not\n    explicitly specified, then we auto-select a model based on the parameters\n    in the RecognitionConfig.\n    \ntable\n\n      \ntr\n\n        \ntd\nb\nModel\n/b\n/td\n\n        \ntd\nb\nDescription\n/b\n/td\n\n      \n/tr\n\n      \ntr\n\n        \ntd\ncode\ncommand_and_search\n/code\n/td\n\n        \ntd\nBest for short queries such as voice commands or voice search.\n/td\n\n      \n/tr\n\n      \ntr\n\n        \ntd\ncode\nphone_call\n/code\n/td\n\n        \ntd\nBest for audio that originated from a phone call (typically\n        recorded at an 8khz sampling rate).\n/td\n\n      \n/tr\n\n      \ntr\n\n        \ntd\ncode\nvideo\n/code\n/td\n\n        \ntd\nBest for audio that originated from from video or includes multiple\n            speakers. Ideally the audio is recorded at a 16khz or greater\n            sampling rate. This is a premium model that costs more than the\n            standard rate.\n/td\n\n      \n/tr\n\n      \ntr\n\n        \ntd\ncode\ndefault\n/code\n/td\n\n        \ntd\nBest for audio that is not one of the specific audio models.\n            For example, long-form audio. Ideally the audio is high-fidelity,\n            recorded at a 16khz or greater sampling rate.\n/td\n\n      \n/tr\n\n    \n/table\n\n\n\n\n\n\nprofanity-filter=false\n\n\nOptional\n If set to \ntrue\n, the server will attempt to filter out\n    profanities, replacing all but the initial character in each filtered word\n    with asterisks, e.g. \nf***\n. If set to \nfalse\n or omitted, profanities\n    won\nt be filtered out.\n\n\n\n\n\n\nsample-rate-hertz=40\n\n\nSample rate in Hertz of the audio data sent in all\n    \nRecognitionAudio\n messages. Valid values are: 8000-48000.\n    16000 is optimal. For best results, set the sampling rate of the audio\n    source to 16000 Hz. If that\ns not possible, use the native sample rate of\n    the audio source (instead of re-sampling).\n    This field is optional for FLAC and WAV audio files, but is\n    required for all other audio formats. For details, see AudioEncoding.\n\n\n\n\n\n\nuse-enhanced=true\n\n\n\n\nOptional\n Set to true to use an enhanced model for speech recognition.\n    If \nuse_enhanced\n is set to true and the \nmodel\n field is not set, then\n    an appropriate enhanced model is chosen if an enhanced model exists for\n    the audio.\n\n\nIf \nuse_enhanced\n is true and an enhanced version of the specified model\ndoes not exist, then the speech is recognized using the standard version\nof the specified model.\n\n\n\n\n\n\n\n\n\n\nAbout Cursors\n\n\nThe cursor position is key to comfortably set complex nested structures. The following rules apply:\n\n\n\n\nThe cursor position is always set relative to the current one, unless the field name starts with the \n.\n character. Fields can be nested such as in \n-r f.s.o\n .\n\n\nThe cursor position is set relative to the top-level structure if it starts with \n.\n, e.g. \n-r .s.s\n\n\nYou can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify \n-r struct.sub_struct=bar\n.\n\n\nYou can move the cursor one level up by using \n..\n. Each additional \n.\n moves it up one additional level. E.g. \n...\n would go three levels up.\n\n\n\n\nOptional Output Flags\n\n\nThe method's return value a JSON encoded structure, which will be written to standard output by default.\n\n\n\n\n-o out\n\n\nout\n specifies the \ndestination\n to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The \ndestination\n may be \n-\n to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.\n\n\n\n\n\n\n\n\nOptional General Properties\n\n\nThe following properties can configure any call, and are not specific to this method.\n\n\n\n\n\n\n-p $-xgafv=string\n\n\n\n\nV1 error format.\n\n\n\n\n\n\n\n\n-p access-token=string\n\n\n\n\nOAuth access token.\n\n\n\n\n\n\n\n\n-p alt=string\n\n\n\n\nData format for response.\n\n\n\n\n\n\n\n\n-p callback=string\n\n\n\n\nJSONP\n\n\n\n\n\n\n\n\n-p fields=string\n\n\n\n\nSelector specifying which fields to include in a partial response.\n\n\n\n\n\n\n\n\n-p key=string\n\n\n\n\nAPI key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.\n\n\n\n\n\n\n\n\n-p oauth-token=string\n\n\n\n\nOAuth 2.0 token for the current user.\n\n\n\n\n\n\n\n\n-p pretty-print=boolean\n\n\n\n\nReturns response with indentations and line breaks.\n\n\n\n\n\n\n\n\n-p quota-user=string\n\n\n\n\nAvailable to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.\n\n\n\n\n\n\n\n\n-p upload-type=string\n\n\n\n\nLegacy upload protocol for media (e.g. \nmedia\n, \nmultipart\n).\n\n\n\n\n\n\n\n\n-p upload-protocol=string\n\n\n\n\nUpload protocol for media (e.g. \nraw\n, \nmultipart\n).", 
            "title": "Longrunningrecognize"
        }, 
        {
            "location": "/speech_longrunningrecognize/#scopes", 
            "text": "You will need authorization for the  https://www.googleapis.com/auth/cloud-platform  scope to make a valid call.  If unset, the scope for this method defaults to  https://www.googleapis.com/auth/cloud-platform .\nYou can set the scope for this method like this:  speech1 --scope  scope  speech longrunningrecognize ...", 
            "title": "Scopes"
        }, 
        {
            "location": "/speech_longrunningrecognize/#required-request-value", 
            "text": "The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure.\nIn the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely.  For example, a structure like this:  LongRunningRecognizeRequest:\n  audio:\n    content: string\n    uri: string\n  config:\n    audio-channel-count: integer\n    enable-automatic-punctuation: boolean\n    enable-separate-recognition-per-channel: boolean\n    enable-word-time-offsets: boolean\n    encoding: string\n    language-code: string\n    max-alternatives: integer\n    metadata:\n      audio-topic: string\n      industry-naics-code-of-audio: integer\n      interaction-type: string\n      microphone-distance: string\n      obfuscated-id: string\n      original-media-type: string\n      original-mime-type: string\n      recording-device-name: string\n      recording-device-type: string\n    model: string\n    profanity-filter: boolean\n    sample-rate-hertz: integer\n    use-enhanced: boolean  can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time.   -r .audio    content=eirmod  The audio data bytes encoded as specified in\n     RecognitionConfig . Note: as with all bytes fields, proto buffers use a\n    pure binary representation, whereas JSON representations use base64.     uri=sit   URI that points to a file that contains audio data bytes as specified in\n     RecognitionConfig . The file must not be compressed (for example, gzip).\n    Currently, only Google Cloud Storage URIs are\n    supported, which must be specified in the following format:\n     gs://bucket_name/object_name  (other URI formats return\n    google.rpc.Code.INVALID_ARGUMENT). For more information, see\n     Request URIs .     ..config    audio-channel-count=36   Optional  The number of channels in the input audio data.\n    ONLY set this for MULTI-CHANNEL recognition.\n    Valid values for LINEAR16 and FLAC are  1 - 8 .\n    Valid values for OGG_OPUS are  1 - 254 .\n    Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only  1 .\n    If  0  or omitted, defaults to one channel (mono).\n    Note: We only recognize the first channel by default.\n    To perform independent recognition on each channel set\n     enable_separate_recognition_per_channel  to  true .    enable-automatic-punctuation=true  Optional  If  true , adds punctuation to recognition result hypotheses.\n    This feature is only available in select languages. Setting this for\n    requests in other languages has no effect at all.\n    The default  false  value does not add punctuation to result hypotheses.\n    Note: This is currently offered as an experimental service, complimentary\n    to all users. In the future this may be exclusively available as a\n    premium feature.    enable-separate-recognition-per-channel=false  This needs to be set to  true  explicitly and  audio_channel_count    1\n    to get each channel recognized separately. The recognition result will\n    contain a  channel_tag  field to state which channel that result belongs\n    to. If this is not true, we will only recognize the first channel. The\n    request is billed cumulatively for all channels recognized:\n     audio_channel_count  multiplied by the length of the audio.    enable-word-time-offsets=true  Optional  If  true , the top result includes a list of words and\n    the start and end time offsets (timestamps) for those words. If\n     false , no word-level time offset information is returned. The default is\n     false .    encoding=kasd  Encoding of audio data sent in all  RecognitionAudio  messages.\n    This field is optional for  FLAC  and  WAV  audio files and required\n    for all other audio formats. For details, see AudioEncoding.    language-code=accusam  Required  The language of the supplied audio as a\n     BCP-47  language tag.\n    Example:  en-US .\n    See  Language Support \n    for a list of the currently supported language codes.    max-alternatives=93  Optional  Maximum number of recognition hypotheses to be returned.\n    Specifically, the maximum number of  SpeechRecognitionAlternative  messages\n    within each  SpeechRecognitionResult .\n    The server may return fewer than  max_alternatives .\n    Valid values are  0 - 30 . A value of  0  or  1  will return a maximum of\n    one. If omitted, will return a maximum of one.    metadata    audio-topic=justo  Description of the content. Eg.  Recordings of federal supreme court\n    hearings from 2012 .    industry-naics-code-of-audio=100  The industry vertical to which this speech recognition request most\n    closely applies. This is most indicative of the topics contained\n    in the audio.  Use the 6-digit NAICS code to identify the industry\n    vertical - see https://www.naics.com/search/.    interaction-type=erat  The use case most closely describing the audio content to be recognized.    microphone-distance=labore  The audio type that most closely describes the audio being recognized.    obfuscated-id=sea  Obfuscated (privacy-protected) ID of the user, to identify number of\n    unique users using the service.    original-media-type=nonumy  The original media the speech was recorded on.    original-mime-type=dolores  Mime type of the original audio file.  For example  audio/m4a ,\n     audio/x-alaw-basic ,  audio/mp3 ,  audio/3gpp .\n    A list of possible audio mime types is maintained at\n    http://www.iana.org/assignments/media-types/media-types.xhtml#audio    recording-device-name=gubergren  The device used to make the recording.  Examples  Nexus 5X  or\n     Polycom SoundStation IP 6000  or  POTS  or  VoIP  or\n     Cardioid Microphone .     recording-device-type=sadipscing   The type of device the speech was recorded with.     ..    model=aliquyam   Optional  Which model to select for the given request. Select the model\n    best suited to your domain to get best results. If a model is not\n    explicitly specified, then we auto-select a model based on the parameters\n    in the RecognitionConfig.\n     table \n       tr \n         td b Model /b /td \n         td b Description /b /td \n       /tr \n       tr \n         td code command_and_search /code /td \n         td Best for short queries such as voice commands or voice search. /td \n       /tr \n       tr \n         td code phone_call /code /td \n         td Best for audio that originated from a phone call (typically\n        recorded at an 8khz sampling rate). /td \n       /tr \n       tr \n         td code video /code /td \n         td Best for audio that originated from from video or includes multiple\n            speakers. Ideally the audio is recorded at a 16khz or greater\n            sampling rate. This is a premium model that costs more than the\n            standard rate. /td \n       /tr \n       tr \n         td code default /code /td \n         td Best for audio that is not one of the specific audio models.\n            For example, long-form audio. Ideally the audio is high-fidelity,\n            recorded at a 16khz or greater sampling rate. /td \n       /tr \n     /table    profanity-filter=false  Optional  If set to  true , the server will attempt to filter out\n    profanities, replacing all but the initial character in each filtered word\n    with asterisks, e.g.  f*** . If set to  false  or omitted, profanities\n    won t be filtered out.    sample-rate-hertz=40  Sample rate in Hertz of the audio data sent in all\n     RecognitionAudio  messages. Valid values are: 8000-48000.\n    16000 is optimal. For best results, set the sampling rate of the audio\n    source to 16000 Hz. If that s not possible, use the native sample rate of\n    the audio source (instead of re-sampling).\n    This field is optional for FLAC and WAV audio files, but is\n    required for all other audio formats. For details, see AudioEncoding.    use-enhanced=true   Optional  Set to true to use an enhanced model for speech recognition.\n    If  use_enhanced  is set to true and the  model  field is not set, then\n    an appropriate enhanced model is chosen if an enhanced model exists for\n    the audio.  If  use_enhanced  is true and an enhanced version of the specified model\ndoes not exist, then the speech is recognized using the standard version\nof the specified model.", 
            "title": "Required Request Value"
        }, 
        {
            "location": "/speech_longrunningrecognize/#about-cursors", 
            "text": "The cursor position is key to comfortably set complex nested structures. The following rules apply:   The cursor position is always set relative to the current one, unless the field name starts with the  .  character. Fields can be nested such as in  -r f.s.o  .  The cursor position is set relative to the top-level structure if it starts with  . , e.g.  -r .s.s  You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify  -r struct.sub_struct=bar .  You can move the cursor one level up by using  .. . Each additional  .  moves it up one additional level. E.g.  ...  would go three levels up.", 
            "title": "About Cursors"
        }, 
        {
            "location": "/speech_longrunningrecognize/#optional-output-flags", 
            "text": "The method's return value a JSON encoded structure, which will be written to standard output by default.   -o out  out  specifies the  destination  to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The  destination  may be  -  to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.", 
            "title": "Optional Output Flags"
        }, 
        {
            "location": "/speech_longrunningrecognize/#optional-general-properties", 
            "text": "The following properties can configure any call, and are not specific to this method.    -p $-xgafv=string   V1 error format.     -p access-token=string   OAuth access token.     -p alt=string   Data format for response.     -p callback=string   JSONP     -p fields=string   Selector specifying which fields to include in a partial response.     -p key=string   API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.     -p oauth-token=string   OAuth 2.0 token for the current user.     -p pretty-print=boolean   Returns response with indentations and line breaks.     -p quota-user=string   Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.     -p upload-type=string   Legacy upload protocol for media (e.g.  media ,  multipart ).     -p upload-protocol=string   Upload protocol for media (e.g.  raw ,  multipart ).", 
            "title": "Optional General Properties"
        }, 
        {
            "location": "/speech_recognize/", 
            "text": "Performs synchronous speech recognition: receive results after all audio\nhas been sent and processed.\n\n\nScopes\n\n\nYou will need authorization for the \nhttps://www.googleapis.com/auth/cloud-platform\n scope to make a valid call.\n\n\nIf unset, the scope for this method defaults to \nhttps://www.googleapis.com/auth/cloud-platform\n.\nYou can set the scope for this method like this: \nspeech1 --scope \nscope\n speech recognize ...\n\n\nRequired Request Value\n\n\nThe request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure.\nIn the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely.\n\n\nFor example, a structure like this:\n\n\nRecognizeRequest:\n  audio:\n    content: string\n    uri: string\n  config:\n    audio-channel-count: integer\n    enable-automatic-punctuation: boolean\n    enable-separate-recognition-per-channel: boolean\n    enable-word-time-offsets: boolean\n    encoding: string\n    language-code: string\n    max-alternatives: integer\n    metadata:\n      audio-topic: string\n      industry-naics-code-of-audio: integer\n      interaction-type: string\n      microphone-distance: string\n      obfuscated-id: string\n      original-media-type: string\n      original-mime-type: string\n      recording-device-name: string\n      recording-device-type: string\n    model: string\n    profanity-filter: boolean\n    sample-rate-hertz: integer\n    use-enhanced: boolean\n\n\n\n\n\ncan be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time.\n\n\n\n\n-r .audio    content=justo\n\n\nThe audio data bytes encoded as specified in\n    \nRecognitionConfig\n. Note: as with all bytes fields, proto buffers use a\n    pure binary representation, whereas JSON representations use base64.\n\n\n\n\n\n\n\n\nuri=et\n\n\n\n\nURI that points to a file that contains audio data bytes as specified in\n    \nRecognitionConfig\n. The file must not be compressed (for example, gzip).\n    Currently, only Google Cloud Storage URIs are\n    supported, which must be specified in the following format:\n    \ngs://bucket_name/object_name\n (other URI formats return\n    google.rpc.Code.INVALID_ARGUMENT). For more information, see\n    \nRequest URIs\n.\n\n\n\n\n\n\n\n\n..config    audio-channel-count=84\n\n\n\n\nOptional\n The number of channels in the input audio data.\n    ONLY set this for MULTI-CHANNEL recognition.\n    Valid values for LINEAR16 and FLAC are \n1\n-\n8\n.\n    Valid values for OGG_OPUS are \n1\n-\n254\n.\n    Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only \n1\n.\n    If \n0\n or omitted, defaults to one channel (mono).\n    Note: We only recognize the first channel by default.\n    To perform independent recognition on each channel set\n    \nenable_separate_recognition_per_channel\n to \ntrue\n.\n\n\n\n\n\n\nenable-automatic-punctuation=true\n\n\nOptional\n If \ntrue\n, adds punctuation to recognition result hypotheses.\n    This feature is only available in select languages. Setting this for\n    requests in other languages has no effect at all.\n    The default \nfalse\n value does not add punctuation to result hypotheses.\n    Note: This is currently offered as an experimental service, complimentary\n    to all users. In the future this may be exclusively available as a\n    premium feature.\n\n\n\n\n\n\nenable-separate-recognition-per-channel=false\n\n\nThis needs to be set to \ntrue\n explicitly and \naudio_channel_count\n \n 1\n    to get each channel recognized separately. The recognition result will\n    contain a \nchannel_tag\n field to state which channel that result belongs\n    to. If this is not true, we will only recognize the first channel. The\n    request is billed cumulatively for all channels recognized:\n    \naudio_channel_count\n multiplied by the length of the audio.\n\n\n\n\n\n\nenable-word-time-offsets=true\n\n\nOptional\n If \ntrue\n, the top result includes a list of words and\n    the start and end time offsets (timestamps) for those words. If\n    \nfalse\n, no word-level time offset information is returned. The default is\n    \nfalse\n.\n\n\n\n\n\n\nencoding=et\n\n\nEncoding of audio data sent in all \nRecognitionAudio\n messages.\n    This field is optional for \nFLAC\n and \nWAV\n audio files and required\n    for all other audio formats. For details, see AudioEncoding.\n\n\n\n\n\n\nlanguage-code=duo\n\n\nRequired\n The language of the supplied audio as a\n    \nBCP-47\n language tag.\n    Example: \nen-US\n.\n    See \nLanguage Support\n\n    for a list of the currently supported language codes.\n\n\n\n\n\n\nmax-alternatives=69\n\n\nOptional\n Maximum number of recognition hypotheses to be returned.\n    Specifically, the maximum number of \nSpeechRecognitionAlternative\n messages\n    within each \nSpeechRecognitionResult\n.\n    The server may return fewer than \nmax_alternatives\n.\n    Valid values are \n0\n-\n30\n. A value of \n0\n or \n1\n will return a maximum of\n    one. If omitted, will return a maximum of one.\n\n\n\n\n\n\nmetadata    audio-topic=sea\n\n\nDescription of the content. Eg. \nRecordings of federal supreme court\n    hearings from 2012\n.\n\n\n\n\n\n\nindustry-naics-code-of-audio=46\n\n\nThe industry vertical to which this speech recognition request most\n    closely applies. This is most indicative of the topics contained\n    in the audio.  Use the 6-digit NAICS code to identify the industry\n    vertical - see https://www.naics.com/search/.\n\n\n\n\n\n\ninteraction-type=eos\n\n\nThe use case most closely describing the audio content to be recognized.\n\n\n\n\n\n\nmicrophone-distance=erat\n\n\nThe audio type that most closely describes the audio being recognized.\n\n\n\n\n\n\nobfuscated-id=sadipscing\n\n\nObfuscated (privacy-protected) ID of the user, to identify number of\n    unique users using the service.\n\n\n\n\n\n\noriginal-media-type=dolor\n\n\nThe original media the speech was recorded on.\n\n\n\n\n\n\noriginal-mime-type=eirmod\n\n\nMime type of the original audio file.  For example \naudio/m4a\n,\n    \naudio/x-alaw-basic\n, \naudio/mp3\n, \naudio/3gpp\n.\n    A list of possible audio mime types is maintained at\n    http://www.iana.org/assignments/media-types/media-types.xhtml#audio\n\n\n\n\n\n\nrecording-device-name=elitr\n\n\nThe device used to make the recording.  Examples \nNexus 5X\n or\n    \nPolycom SoundStation IP 6000\n or \nPOTS\n or \nVoIP\n or\n    \nCardioid Microphone\n.\n\n\n\n\n\n\n\n\nrecording-device-type=amet\n\n\n\n\nThe type of device the speech was recorded with.\n\n\n\n\n\n\n\n\n..    model=no\n\n\n\n\nOptional\n Which model to select for the given request. Select the model\n    best suited to your domain to get best results. If a model is not\n    explicitly specified, then we auto-select a model based on the parameters\n    in the RecognitionConfig.\n    \ntable\n\n      \ntr\n\n        \ntd\nb\nModel\n/b\n/td\n\n        \ntd\nb\nDescription\n/b\n/td\n\n      \n/tr\n\n      \ntr\n\n        \ntd\ncode\ncommand_and_search\n/code\n/td\n\n        \ntd\nBest for short queries such as voice commands or voice search.\n/td\n\n      \n/tr\n\n      \ntr\n\n        \ntd\ncode\nphone_call\n/code\n/td\n\n        \ntd\nBest for audio that originated from a phone call (typically\n        recorded at an 8khz sampling rate).\n/td\n\n      \n/tr\n\n      \ntr\n\n        \ntd\ncode\nvideo\n/code\n/td\n\n        \ntd\nBest for audio that originated from from video or includes multiple\n            speakers. Ideally the audio is recorded at a 16khz or greater\n            sampling rate. This is a premium model that costs more than the\n            standard rate.\n/td\n\n      \n/tr\n\n      \ntr\n\n        \ntd\ncode\ndefault\n/code\n/td\n\n        \ntd\nBest for audio that is not one of the specific audio models.\n            For example, long-form audio. Ideally the audio is high-fidelity,\n            recorded at a 16khz or greater sampling rate.\n/td\n\n      \n/tr\n\n    \n/table\n\n\n\n\n\n\nprofanity-filter=true\n\n\nOptional\n If set to \ntrue\n, the server will attempt to filter out\n    profanities, replacing all but the initial character in each filtered word\n    with asterisks, e.g. \nf***\n. If set to \nfalse\n or omitted, profanities\n    won\nt be filtered out.\n\n\n\n\n\n\nsample-rate-hertz=62\n\n\nSample rate in Hertz of the audio data sent in all\n    \nRecognitionAudio\n messages. Valid values are: 8000-48000.\n    16000 is optimal. For best results, set the sampling rate of the audio\n    source to 16000 Hz. If that\ns not possible, use the native sample rate of\n    the audio source (instead of re-sampling).\n    This field is optional for FLAC and WAV audio files, but is\n    required for all other audio formats. For details, see AudioEncoding.\n\n\n\n\n\n\nuse-enhanced=true\n\n\n\n\nOptional\n Set to true to use an enhanced model for speech recognition.\n    If \nuse_enhanced\n is set to true and the \nmodel\n field is not set, then\n    an appropriate enhanced model is chosen if an enhanced model exists for\n    the audio.\n\n\nIf \nuse_enhanced\n is true and an enhanced version of the specified model\ndoes not exist, then the speech is recognized using the standard version\nof the specified model.\n\n\n\n\n\n\n\n\n\n\nAbout Cursors\n\n\nThe cursor position is key to comfortably set complex nested structures. The following rules apply:\n\n\n\n\nThe cursor position is always set relative to the current one, unless the field name starts with the \n.\n character. Fields can be nested such as in \n-r f.s.o\n .\n\n\nThe cursor position is set relative to the top-level structure if it starts with \n.\n, e.g. \n-r .s.s\n\n\nYou can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify \n-r struct.sub_struct=bar\n.\n\n\nYou can move the cursor one level up by using \n..\n. Each additional \n.\n moves it up one additional level. E.g. \n...\n would go three levels up.\n\n\n\n\nOptional Output Flags\n\n\nThe method's return value a JSON encoded structure, which will be written to standard output by default.\n\n\n\n\n-o out\n\n\nout\n specifies the \ndestination\n to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The \ndestination\n may be \n-\n to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.\n\n\n\n\n\n\n\n\nOptional General Properties\n\n\nThe following properties can configure any call, and are not specific to this method.\n\n\n\n\n\n\n-p $-xgafv=string\n\n\n\n\nV1 error format.\n\n\n\n\n\n\n\n\n-p access-token=string\n\n\n\n\nOAuth access token.\n\n\n\n\n\n\n\n\n-p alt=string\n\n\n\n\nData format for response.\n\n\n\n\n\n\n\n\n-p callback=string\n\n\n\n\nJSONP\n\n\n\n\n\n\n\n\n-p fields=string\n\n\n\n\nSelector specifying which fields to include in a partial response.\n\n\n\n\n\n\n\n\n-p key=string\n\n\n\n\nAPI key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.\n\n\n\n\n\n\n\n\n-p oauth-token=string\n\n\n\n\nOAuth 2.0 token for the current user.\n\n\n\n\n\n\n\n\n-p pretty-print=boolean\n\n\n\n\nReturns response with indentations and line breaks.\n\n\n\n\n\n\n\n\n-p quota-user=string\n\n\n\n\nAvailable to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.\n\n\n\n\n\n\n\n\n-p upload-type=string\n\n\n\n\nLegacy upload protocol for media (e.g. \nmedia\n, \nmultipart\n).\n\n\n\n\n\n\n\n\n-p upload-protocol=string\n\n\n\n\nUpload protocol for media (e.g. \nraw\n, \nmultipart\n).", 
            "title": "Recognize"
        }, 
        {
            "location": "/speech_recognize/#scopes", 
            "text": "You will need authorization for the  https://www.googleapis.com/auth/cloud-platform  scope to make a valid call.  If unset, the scope for this method defaults to  https://www.googleapis.com/auth/cloud-platform .\nYou can set the scope for this method like this:  speech1 --scope  scope  speech recognize ...", 
            "title": "Scopes"
        }, 
        {
            "location": "/speech_recognize/#required-request-value", 
            "text": "The request value is a data-structure with various fields. Each field may be a simple scalar or another data-structure.\nIn the latter case it is advised to set the field-cursor to the data-structure's field to specify values more concisely.  For example, a structure like this:  RecognizeRequest:\n  audio:\n    content: string\n    uri: string\n  config:\n    audio-channel-count: integer\n    enable-automatic-punctuation: boolean\n    enable-separate-recognition-per-channel: boolean\n    enable-word-time-offsets: boolean\n    encoding: string\n    language-code: string\n    max-alternatives: integer\n    metadata:\n      audio-topic: string\n      industry-naics-code-of-audio: integer\n      interaction-type: string\n      microphone-distance: string\n      obfuscated-id: string\n      original-media-type: string\n      original-mime-type: string\n      recording-device-name: string\n      recording-device-type: string\n    model: string\n    profanity-filter: boolean\n    sample-rate-hertz: integer\n    use-enhanced: boolean  can be set completely with the following arguments which are assumed to be executed in the given order. Note how the cursor position is adjusted to the respective structures, allowing simple field names to be used most of the time.   -r .audio    content=justo  The audio data bytes encoded as specified in\n     RecognitionConfig . Note: as with all bytes fields, proto buffers use a\n    pure binary representation, whereas JSON representations use base64.     uri=et   URI that points to a file that contains audio data bytes as specified in\n     RecognitionConfig . The file must not be compressed (for example, gzip).\n    Currently, only Google Cloud Storage URIs are\n    supported, which must be specified in the following format:\n     gs://bucket_name/object_name  (other URI formats return\n    google.rpc.Code.INVALID_ARGUMENT). For more information, see\n     Request URIs .     ..config    audio-channel-count=84   Optional  The number of channels in the input audio data.\n    ONLY set this for MULTI-CHANNEL recognition.\n    Valid values for LINEAR16 and FLAC are  1 - 8 .\n    Valid values for OGG_OPUS are  1 - 254 .\n    Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only  1 .\n    If  0  or omitted, defaults to one channel (mono).\n    Note: We only recognize the first channel by default.\n    To perform independent recognition on each channel set\n     enable_separate_recognition_per_channel  to  true .    enable-automatic-punctuation=true  Optional  If  true , adds punctuation to recognition result hypotheses.\n    This feature is only available in select languages. Setting this for\n    requests in other languages has no effect at all.\n    The default  false  value does not add punctuation to result hypotheses.\n    Note: This is currently offered as an experimental service, complimentary\n    to all users. In the future this may be exclusively available as a\n    premium feature.    enable-separate-recognition-per-channel=false  This needs to be set to  true  explicitly and  audio_channel_count    1\n    to get each channel recognized separately. The recognition result will\n    contain a  channel_tag  field to state which channel that result belongs\n    to. If this is not true, we will only recognize the first channel. The\n    request is billed cumulatively for all channels recognized:\n     audio_channel_count  multiplied by the length of the audio.    enable-word-time-offsets=true  Optional  If  true , the top result includes a list of words and\n    the start and end time offsets (timestamps) for those words. If\n     false , no word-level time offset information is returned. The default is\n     false .    encoding=et  Encoding of audio data sent in all  RecognitionAudio  messages.\n    This field is optional for  FLAC  and  WAV  audio files and required\n    for all other audio formats. For details, see AudioEncoding.    language-code=duo  Required  The language of the supplied audio as a\n     BCP-47  language tag.\n    Example:  en-US .\n    See  Language Support \n    for a list of the currently supported language codes.    max-alternatives=69  Optional  Maximum number of recognition hypotheses to be returned.\n    Specifically, the maximum number of  SpeechRecognitionAlternative  messages\n    within each  SpeechRecognitionResult .\n    The server may return fewer than  max_alternatives .\n    Valid values are  0 - 30 . A value of  0  or  1  will return a maximum of\n    one. If omitted, will return a maximum of one.    metadata    audio-topic=sea  Description of the content. Eg.  Recordings of federal supreme court\n    hearings from 2012 .    industry-naics-code-of-audio=46  The industry vertical to which this speech recognition request most\n    closely applies. This is most indicative of the topics contained\n    in the audio.  Use the 6-digit NAICS code to identify the industry\n    vertical - see https://www.naics.com/search/.    interaction-type=eos  The use case most closely describing the audio content to be recognized.    microphone-distance=erat  The audio type that most closely describes the audio being recognized.    obfuscated-id=sadipscing  Obfuscated (privacy-protected) ID of the user, to identify number of\n    unique users using the service.    original-media-type=dolor  The original media the speech was recorded on.    original-mime-type=eirmod  Mime type of the original audio file.  For example  audio/m4a ,\n     audio/x-alaw-basic ,  audio/mp3 ,  audio/3gpp .\n    A list of possible audio mime types is maintained at\n    http://www.iana.org/assignments/media-types/media-types.xhtml#audio    recording-device-name=elitr  The device used to make the recording.  Examples  Nexus 5X  or\n     Polycom SoundStation IP 6000  or  POTS  or  VoIP  or\n     Cardioid Microphone .     recording-device-type=amet   The type of device the speech was recorded with.     ..    model=no   Optional  Which model to select for the given request. Select the model\n    best suited to your domain to get best results. If a model is not\n    explicitly specified, then we auto-select a model based on the parameters\n    in the RecognitionConfig.\n     table \n       tr \n         td b Model /b /td \n         td b Description /b /td \n       /tr \n       tr \n         td code command_and_search /code /td \n         td Best for short queries such as voice commands or voice search. /td \n       /tr \n       tr \n         td code phone_call /code /td \n         td Best for audio that originated from a phone call (typically\n        recorded at an 8khz sampling rate). /td \n       /tr \n       tr \n         td code video /code /td \n         td Best for audio that originated from from video or includes multiple\n            speakers. Ideally the audio is recorded at a 16khz or greater\n            sampling rate. This is a premium model that costs more than the\n            standard rate. /td \n       /tr \n       tr \n         td code default /code /td \n         td Best for audio that is not one of the specific audio models.\n            For example, long-form audio. Ideally the audio is high-fidelity,\n            recorded at a 16khz or greater sampling rate. /td \n       /tr \n     /table    profanity-filter=true  Optional  If set to  true , the server will attempt to filter out\n    profanities, replacing all but the initial character in each filtered word\n    with asterisks, e.g.  f*** . If set to  false  or omitted, profanities\n    won t be filtered out.    sample-rate-hertz=62  Sample rate in Hertz of the audio data sent in all\n     RecognitionAudio  messages. Valid values are: 8000-48000.\n    16000 is optimal. For best results, set the sampling rate of the audio\n    source to 16000 Hz. If that s not possible, use the native sample rate of\n    the audio source (instead of re-sampling).\n    This field is optional for FLAC and WAV audio files, but is\n    required for all other audio formats. For details, see AudioEncoding.    use-enhanced=true   Optional  Set to true to use an enhanced model for speech recognition.\n    If  use_enhanced  is set to true and the  model  field is not set, then\n    an appropriate enhanced model is chosen if an enhanced model exists for\n    the audio.  If  use_enhanced  is true and an enhanced version of the specified model\ndoes not exist, then the speech is recognized using the standard version\nof the specified model.", 
            "title": "Required Request Value"
        }, 
        {
            "location": "/speech_recognize/#about-cursors", 
            "text": "The cursor position is key to comfortably set complex nested structures. The following rules apply:   The cursor position is always set relative to the current one, unless the field name starts with the  .  character. Fields can be nested such as in  -r f.s.o  .  The cursor position is set relative to the top-level structure if it starts with  . , e.g.  -r .s.s  You can also set nested fields without setting the cursor explicitly. For example, to set a value relative to the current cursor position, you would specify  -r struct.sub_struct=bar .  You can move the cursor one level up by using  .. . Each additional  .  moves it up one additional level. E.g.  ...  would go three levels up.", 
            "title": "About Cursors"
        }, 
        {
            "location": "/speech_recognize/#optional-output-flags", 
            "text": "The method's return value a JSON encoded structure, which will be written to standard output by default.   -o out  out  specifies the  destination  to which to write the server's result to.\n  It will be a JSON-encoded structure.\n  The  destination  may be  -  to indicate standard output, or a filepath that is to contain the received bytes.\n  If unset, it defaults to standard output.", 
            "title": "Optional Output Flags"
        }, 
        {
            "location": "/speech_recognize/#optional-general-properties", 
            "text": "The following properties can configure any call, and are not specific to this method.    -p $-xgafv=string   V1 error format.     -p access-token=string   OAuth access token.     -p alt=string   Data format for response.     -p callback=string   JSONP     -p fields=string   Selector specifying which fields to include in a partial response.     -p key=string   API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.     -p oauth-token=string   OAuth 2.0 token for the current user.     -p pretty-print=boolean   Returns response with indentations and line breaks.     -p quota-user=string   Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.     -p upload-type=string   Legacy upload protocol for media (e.g.  media ,  multipart ).     -p upload-protocol=string   Upload protocol for media (e.g.  raw ,  multipart ).", 
            "title": "Optional General Properties"
        }
    ]
}